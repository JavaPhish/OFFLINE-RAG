# Local RAG LLM with Ollama

A fully self-hosted, offline RAG (Retrieval-Augmented Generation) chat application built with LangChain, Ollama, and React. Query your personal documents without sending data to the cloud.

**Your data. Your control. Your privacy.**

## ğŸ”’ Privacy & Control First

This project is built with **data sovereignty** as the core principle:

- **Zero Cloud Uploads** â€” All documents, embeddings, and conversations stay on your machine. Not a single byte leaves your device.
- **No Tracking** â€” No analytics, no telemetry, no third-party services. Complete privacy by default.
- **Full Ownership** â€” You own your data, your models, your vector store. Run it your way, modify it, or delete it anytime.
- **No Vendor Lock-in** â€” Use any Ollama model. Switch models, embedding models, or vector stores without external dependencies.
- **Open Source** â€” Inspect the code, audit security, contribute improvements. No closed-source black boxes.

## Features

- **100% Local & Offline** â€” All processing runs on your machine. No API calls, no cloud storage.
- **RAG (Retrieval-Augmented Generation)** â€” Augment the LLM with your personal document knowledge base.
- **Chat Sessions** â€” Maintain multiple conversation threads with persistent storage.
- **Cross-Chat References** â€” Reference and synthesize insights from previous conversations.
- **Advanced LLM Controls** â€” Fine-tune temperature, top-p, top-k, seed, and other generation parameters.
- **RAG Toggle** â€” Switch between knowledge-base mode and base LLM mode to save resources.
- **Automatic File Detection** â€” Vector store rebuilds when source files change.
- **Beautiful Dark UI** â€” Modern React frontend with real-time chat.

### Disclaimer:
This project is still in early stages of development, expect the associated tedium that comes with this. I have tried to provide accurate steps to utilize this code, but it is still in a "dev" oriented usage in its current build.

## Screenshots

### Chat Interface with Document Retrieval
![Overall RAG chat application](./docs/RAG%20example.png)

*The main chat interface showing RAG-augmented queries with document sources and cross-chat capabilities.*

### Advanced LLM Parameter Tuning
![Custom model parameter tuning](./docs/tuning.png)

*Fine-grained control over LLM generation with temperature, top-p, top-k, seed, and other advanced options that paid mass market LLMs would not often allow for.*

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (React + Vite)                  â”‚
â”‚                    localhost:5173                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/JSON
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Backend (FastAPI)                           â”‚
â”‚                 localhost:8000                               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Vector Store    â”‚  â”‚  Chat Persistenceâ”‚                â”‚
â”‚  â”‚  (Chroma)        â”‚  â”‚  (JSON Files)    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                                                    â”‚
â”‚         â”œâ”€ Document Loading (PDF, TXT, MD)                 â”‚
â”‚         â”œâ”€ Text Splitting (1000 chars/chunk)               â”‚
â”‚         â””â”€ Embeddings (SentenceTransformer)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Local HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Ollama (Local LLM)                            â”‚
â”‚  gemma3:12b (or your chosen model)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Flow:
  User Query â†’ Frontend â†’ Backend RAG Pipeline â†’ Vector DB Search
           â†“
       Retrieved Context + History + Prompt â†’ Ollama â†’ LLM Response
           â†“
       Assistant Answer + Sources â†’ Frontend (Persistent Chat)
```

## Quick Start

### Prerequisites

- **Python 3.10+**
- **Node.js 18+**
- **Ollama** installed and running locally with a model (e.g., `ollama pull gemma3:12b`)

### Setup

1. **Clone the repo**
   ```bash
   git clone <your-repo-url>
   cd private_rag_llm
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start the backend**
   ```bash
   python -m uvicorn app.main:app --host 127.0.0.1 --port 8000
   ```
   The API will be available at `http://127.0.0.1:8000`

4. **Install and run the frontend** (in a new terminal)
   ```bash
   cd frontend
   npm install
   npm run dev
   ```
   The UI will be available at `http://localhost:5173`

5. **Add your documents**
   Place PDF, TXT, or Markdown files in the `./data/` folder. The vector store will rebuild automatically on the next query.

6. **Start chatting**
   Open the frontend and ask questions about your documents. The system will search your knowledge base and augment the LLM's response with relevant context.

## Configuration

### Backend (`app/config.py`)

- `DATA_DIR` â€” Folder containing your documents (default: `./data/`)
- `CHROMA_DIR` â€” Vector store location (default: `./chroma_db/`)
- `OLLAMA_MODEL` â€” Model name (default: `gemma3:12b`)
- `MODEL_NAME` â€” Embedding model (default: `all-MiniLM-L6-v2`)
- `CHUNK_SIZE` â€” Document chunk size in characters (default: `1000`)
- `CHUNK_OVERLAP` â€” Overlap between chunks (default: `200`)
- `API_TOKEN` â€” Optional Bearer token for API security

### Frontend (`frontend/src/App.jsx`)

- `API_BASE` â€” Backend URL (default: `http://127.0.0.1:8000`)

## API Endpoints

### `/query` (POST)
Submit a question and get an answer from the RAG pipeline.

**Request:**
```json
{
  "query": "What are the main points?",
  "k": 3,
  "use_rag": true,
  "reference_chats": ["chat-id-1"],
  "history": [{"role": "user", "content": "..."}, ...],
  "temperature": 0.7,
  "top_p": 0.9,
  "top_k": 40,
  "max_tokens": 512
}
```

**Response:**
```json
{
  "answer": "Based on the documents...",
  "sources": [
    {
      "source": "/path/to/document.pdf",
      "snippet": "Relevant text excerpt..."
    }
  ]
}
```

### `/chats` (GET, POST, PUT, DELETE)
Manage chat sessions. All chats are persisted to disk in `./chat_store/`.

- `GET /chats` â€” List all chats
- `POST /chats` â€” Create a new chat
- `GET /chats/{chat_id}` â€” Retrieve a specific chat
- `PUT /chats/{chat_id}` â€” Update a chat (save messages)
- `DELETE /chats/{chat_id}` â€” Delete a chat

### `/chats/summary/{chat_id}` (GET)
Get a brief summary of a chat for cross-referencing.

### `/health` (GET)
Health check endpoint.

### `/reindex` (POST)
Force a rebuild of the vector store from source documents.

## File Structure

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI server and RAG pipeline
â”‚   â”œâ”€â”€ indexer.py              # Document loading, chunking, embedding
â”‚   â”œâ”€â”€ chat_store.py           # Chat persistence (JSON files)
â”‚   â”œâ”€â”€ ollama.py               # Ollama integration (CLI + HTTP)
â”‚   â”œâ”€â”€ schemas.py              # Pydantic models
â”‚   â””â”€â”€ config.py               # Configuration
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main React component
â”‚   â”‚   â”œâ”€â”€ main.jsx            # Entry point
â”‚   â”‚   â””â”€â”€ styles.css          # Styling
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ data/                        # Your documents (PDFs, TXT, MD)
â”œâ”€â”€ chroma_db/                   # Vector store (SQLite + embeddings)
â”œâ”€â”€ chat_store/                  # Chat sessions (JSON files)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## Supported Document Formats

- **PDF** â€” Text extraction using `pypdf`
- **TXT** â€” Plain text files
- **MD** â€” Markdown files
- **HTML** â€” HTML files

**Note:** Image-based PDFs (scanned documents) require OCR and are automatically skipped.

## How RAG Works

1. **Document Ingestion** â€” PDFs/TXT/MD files are loaded and split into 1000-character chunks with 200-character overlap.
2. **Embedding** â€” Each chunk is converted to a vector using `SentenceTransformer` (all-MiniLM-L6-v2).
3. **Storage** â€” Vectors and metadata are persisted in Chroma (SQLite + FAISS).
4. **Retrieval** â€” On query, the top-k most similar chunks are retrieved using cosine similarity.
5. **Augmentation** â€” Retrieved context is prepended to the user's query as a prompt.
6. **Generation** â€” Ollama's LLM generates a response based on the augmented prompt.

## LLM Parameters

Fine-tune how the model generates responses:

- **Temperature** (0.0â€“2.0) â€” Controls randomness. Lower = more deterministic.
- **Top-P** (0.0â€“1.0) â€” Nucleus sampling. Limits to cumulative probability mass.
- **Top-K** (1â€“100) â€” Limits token choices to the top K tokens.
- **Repeat Penalty** (0.0â€“2.0) â€” Penalizes repeating tokens to reduce loops.
- **Seed** â€” Fixes randomness to reproduce outputs.
- **Max Tokens** â€” Limits response length.
- **Context Window (num_ctx)** â€” How many tokens the model considers.
- **Mirostat** (0/1/2) â€” Advanced entropy control for stable outputs.

## Chat Sessions & References

The UI maintains multiple chat conversations:
- Each chat is stored as a JSON file in `./chat_store/`.
- Switch between chats in the sidebar.
- Reference past chats to synthesize cross-conversation insights.
- Delete chats with the âœ• button in the sidebar.

## Vector Store Management

The vector store is automatically rebuilt when:
- Source files change (detected via manifest hash comparison).
- The `./chroma_db/` directory is deleted.
- The `/reindex` endpoint is called.

To manually rebuild:
```bash
rm -rf ./chroma_db
python -m uvicorn app.main:app --host 127.0.0.1 --port 8000
```

## Troubleshooting

### Backend won't start
- Ensure Python 3.10+ is installed: `python --version`
- Check Ollama is running: `ollama serve` in another terminal
- Install dependencies: `pip install -r requirements.txt`

### Frontend can't connect to backend
- Verify backend is running on `http://127.0.0.1:8000`
- Check firewall/proxy settings
- Clear browser cache

### Vector store has old data
- Delete `./chroma_db/` and restart the backend
- Or call the `/reindex` endpoint

### Ollama model not found
- List available models: `ollama list`
- Pull a model: `ollama pull gemma3:12b`

### PDFs not indexing
- Ensure PDFs are in the `./data/` folder
- Check if they're text-based or scanned (scanned PDFs are skipped)
- Restart the backend to trigger reindexing

## Performance Tips

- Use a local SSD for faster vector store access.
- Adjust `CHUNK_SIZE` and `CHUNK_OVERLAP` based on your document types.
- Disable RAG mode (`use_rag=false`) if you want faster generation without retrieval.
- Use a smaller, faster model (e.g., `phi:2b`) for quick responses.

## Why Local?

### For Businesses & Organizations
- **Compliance** â€” Meet GDPR, HIPAA, SOX, and other data residency requirements.
- **Confidentiality** â€” Sensitive business documents never leave your infrastructure.
- **Control** â€” No subscription fees, no API rate limits, no vendor dependence.
- **Auditability** â€” Full visibility into what's happening with your data.

### For Individuals
- **Privacy** â€” Keep your personal documents, research, and ideas completely private.
- **Cost** â€” No ongoing API bills. One-time hardware investment.
- **Freedom** â€” Use your own models, tweak algorithms, run exactly as you want.
- **Offline** â€” Works without internet; no connectivity required.

## Security

- **The system is fully local and offline** â€” No data leaves your machine. Ever.
- **No external dependencies** â€” Ollama and embeddings run locally; no API calls to third parties.
- **Optional API token** â€” Set `API_TOKEN` in `app/config.py` to require a Bearer token for calls.
- **Chat persistence** â€” Chats are stored as JSON files locally; protect the `./chat_store/` directory if sensitive.
- **Vector store** â€” Your embeddings are stored in Chroma (SQLite) with full ownership.
- **Source transparency** â€” Code is open source; you can audit, modify, and secure it yourself.

**Important:** While the system is local, if you deploy this over a network:
- Use HTTPS/TLS for transport security
- Implement authentication/authorization
- Restrict network access to trusted clients
- Use VPN or private networks

## Future Enhancements

- [ ] OCR support for scanned PDFs (via Tesseract or cloud API)
- [ ] File upload UI (add documents without restarting)
- [ ] Background file watcher for auto-reindexing
- [ ] Hybrid search (BM25 + semantic)
- [ ] Multi-user support with user accounts
- [ ] Web UI deployment (Docker)
- [ ] End-to-end encryption for network deployments
- [ ] Audit logging for compliance

## TODO

### Containerization & Deployment
- [ ] **Containerize Application** â€” Create Docker image for easy deployment
  - Docker Compose setup for backend + frontend
  - Support for docker-compose.yml with volumes for data persistence
  - Multi-stage build for optimized image size
  - Pre-built images for common Ollama models
  - Add sections for custom context / system prompt controls for even more control over LLM performance and utility

### Remote Access & Security
- [ ] **Expose to Internet with Authentication**
  - OAuth2 / OpenID Connect support
  - Multi-user authentication with session management
  - Role-based access control (RBAC) for different permission levels
  - Rate limiting and DDoS protection
  - HTTPS/TLS certificate management (Let's Encrypt integration)
  - API key management for programmatic access

### Advanced Features
- [ ] **Document Management** â€” UI for uploading/deleting/managing documents without restart
- [ ] **Model Switching** â€” UI to switch between different Ollama models on-the-fly
- [ ] **Search Analytics** â€” Track which queries are most common, document popularity
- [ ] **Export Capabilities** â€” Export chats as markdown/PDF, export vector DB

### Infrastructure
- [ ] **Kubernetes Support** â€” Helm charts for K8s deployment
- [ ] **Database Migration** â€” Support PostgreSQL/MySQL as alternative to SQLite
- [ ] **Backup & Restore** â€” Automated backup of vector store and chats
- [ ] **Monitoring** â€” Prometheus metrics, health checks, logging aggregation

## License

MIT

## Contributing

Contributions welcome! Feel free to open issues or submit PRs.

---

**Questions?** Check the [Backend README](./README_BACKEND.md) and [Frontend README](./frontend/README_FRONTEND.md) for more details.
